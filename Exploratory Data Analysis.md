### Prerequisites:
- Python Programming Language 
- Python Packages
	- Pandas
	- NumPy
	- SciPy
	- SkLearn
	- MatPlotLib
	- Seaborn

## Machine Learning Basics
**Machine Learning (ML)** is a subset of artificial intelligence (AI) that enables computers to learn from data and improve their performance on a task without being explicitly programmed.

### Key Components:
1. **Data**: The raw information used to train and test models.
2. **Algorithms**: Methods or processes that enable learning from data.
3. **Model**: The mathematical representation created by the algorithm based on the data.
4. **Prediction**: The output generated by the model for new, unseen data.

### Core Idea:
Machine learning focuses on creating systems that can identify patterns, make decisions, and improve performance through experience, akin to how humans learn from past experiences.

### Example:
- A machine learning model can analyse historical sales data to predict future trends.
- It learns from past inputs (features like price, season, marketing efforts) and outputs (sales figures) to make accurate predictions.

Machine Learning can be divided into:
1. Supervised
2. Unsupervised
3. Reinforcement

In Machine Learning, in a dataset, a column is called the **Feature**.
							 and a row is called the **Data Point**

In a dataset when both input and output are present in feature, it is called a **Labelled Data**. It is a part of **Supervised ML.**
And a dataset where the output is not present is called a **Unlabelled Data**. It is a part of **Unsupervised ML**.

### Dependent Variable
- **Definition**: The variable you are trying to predict or explain. Its value depends on the changes in one or more independent variables.
- **Also Known As**: Target variable, Output variable, Response variable.
- **Role**: It's the outcome you want your model to learn and predict.
#### Example:
- In predicting house prices:
    - **Dependent Variable**: Price of the house.

### Independent Variable
- **Definition**: The variable(s) used to explain or predict the dependent variable. These are the inputs or features provided to the model.
- **Also Known As**: Predictor variables, Input variables, Features.
- **Role**: They influence or determine the dependent variable.
#### Example:
- In predicting house prices:
    - **Independent Variables**: Square footage, number of bedrooms, location, etc.

In a dataset:
- **Independent Variables**: Columns/features that describe the input data.
- **Dependent Variable**: Column/label that represents the output to be predicted.

#### Example in a Dataset:

| Square Footage | Bedrooms | Location | Price   |
| -------------- | -------- | -------- | ------- |
| 1500           | 3        | Urban    | 200,000 |

- **Independent Variables**: Square Footage, Bedrooms, Location.
- **Dependent Variable**: Price.

### Train Data & Test Data
In machine learning, data is split into two main subsets: **train data** and **test data**. These subsets are crucial for building and evaluating models.

#### Train Data
- **Definition**: The portion of the dataset used to train the machine learning model. The model learns patterns and relationships between input features and the target (dependent) variable from this data.
- **Purpose**: To help the model learn and develop a mathematical representation of the data.
- **Characteristics**:
    - Contains both input features (independent variables) and corresponding output labels (dependent variable).
    - Typically constitutes the majority of the dataset (e.g., 70%-80%).
- **Example**: In a dataset for predicting house prices:
    - Train Data: A subset with known house prices and features like square footage, number of bedrooms, and location.

#### Test Data
- **Definition**: The portion of the dataset used to evaluate the model's performance. It checks how well the model can generalise to unseen data.
- **Purpose**: To measure the model's accuracy, robustness, and ability to handle new data.
- **Characteristics**:
    - Contains the same features as the train data but is not shown to the model during training.
    - Typically constitutes the remaining portion of the dataset (e.g., 20%-30%).
    - Labels are known and used to compare predictions against the actual outcomes.
- **Example**: In the house price dataset:
    - Test Data: A subset with unseen house features and known prices, used to test the model's predictions.

### Supervised Machine Learning
- In supervised learning, the model is trained on a **labelled dataset**, meaning each training example includes input features (independent variables) and their corresponding correct output (dependent variable or label).
- The goal is for the model to learn the mapping from inputs to outputs and predict outcomes for unseen data.

#### Classifications in Supervised ML
1. **Regression:**
	- Predicts a continuous value.
	- Examples:
	    - Predicting house prices based on square footage, number of bedrooms, etc.
	    - Forecasting stock prices.
	- Algorithms:
	    - Linear Regression
	    - Support Vector Regression (SVR)
	    - Decision Tree Regression
2. **Classification**
	- Predicts discrete categories or classes.
	- Examples:
	    - Classifying emails as spam or not spam.
	    - Diagnosing a patient as having a disease or not.
	- Algorithms:
	    - Logistic Regression
	    - Support Vector Machines (SVM)
	    - k-Nearest Neighbours (k-NN)
	    - Decision Trees and Random Forests

### Unsupervised Machine Learning
- In unsupervised learning, the model is trained on **unlabelled data**. The algorithm tries to identify hidden patterns, structures, or relationships in the data without specific guidance.
- The goal is to organise data meaningfully or reduce its dimensionality for easier understanding.

#### Classifications in Unsupervised ML
1. **Clustering**
	- Groups data points into clusters based on their similarity.
	- Examples:
	    - Customer segmentation in marketing.
	    - Organising news articles by topic.
	- Algorithms:
	    - k-Means Clustering
	    - Hierarchical Clustering
	    - DBSCAN (Density-Based Spatial Clustering)
2. **Dimensionality Reduction**
	- Reduces the number of features or variables in the dataset while retaining the most important information.
	- Examples:
	    - Visualising high-dimensional data in 2D or 3D.
	    - Compressing images.
	- Algorithms:
	    - Principal Component Analysis (PCA)
	    - t-Distributed Stochastic Neighbour Embedding (t-SNE)
	    - Singular Value Decomposition (SVD)

### Comparison of Supervised and Unsupervised ML

| **Aspect**           | **Supervised Learning**                         | **Unsupervised Learning**                    |
| -------------------- | ----------------------------------------------- | -------------------------------------------- |
| **Input Data**       | Labeled (input-output pairs)                    | Unlabeled (only input features)              |
| **Goal**             | Predict outcomes (Regression/Classification)    | Identify patterns or structures              |
| **Common Use Cases** | Predictive tasks (e.g., house price prediction) | Exploratory tasks (e.g., grouping customers) |
| **Evaluation**       | Accuracy, Precision, Recall, F1 Score           | Intra-cluster similarity, Explained Variance |
### Outliers
**Outliers** are data points in a dataset that deviate significantly from the other observations. They are unusually high or low values that do not fit the general pattern or trend of the data. These values may arise due to variability in measurement, experimental errors, or genuinely rare events.

#### Characteristics of Outliers
1. They are far away from the majority of the data points.
2. They can distort statistical analyses and machine learning moderns
3. 