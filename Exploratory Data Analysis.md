### Prerequisites:
- Python Programming Language 
- Python Packages
	- Pandas
	- NumPy
	- SciPy
	- SkLearn
	- MatPlotLib
	- Seaborn

## Machine Learning Basics
**Machine Learning (ML)** is a subset of artificial intelligence (AI) that enables computers to learn from data and improve their performance on a task without being explicitly programmed.

### Key Components:
1. **Data**: The raw information used to train and test models.
2. **Algorithms**: Methods or processes that enable learning from data.
3. **Model**: The mathematical representation created by the algorithm based on the data.
4. **Prediction**: The output generated by the model for new, unseen data.

### Core Idea:
Machine learning focuses on creating systems that can identify patterns, make decisions, and improve performance through experience, akin to how humans learn from past experiences.

### Example:
- A machine learning model can analyse historical sales data to predict future trends.
- It learns from past inputs (features like price, season, marketing efforts) and outputs (sales figures) to make accurate predictions.

Machine Learning can be divided into:
1. Supervised
2. Unsupervised
3. Reinforcement

In Machine Learning, in a dataset, a column is called the **Feature**.
							 and a row is called the **Data Point**

In a dataset when both input and output are present in feature, it is called a **Labelled Data**. It is a part of **Supervised ML.**
And a dataset where the output is not present is called a **Unlabelled Data**. It is a part of **Unsupervised ML**.

### Dependent Variable
- **Definition**: The variable you are trying to predict or explain. Its value depends on the changes in one or more independent variables.
- **Also Known As**: Target variable, Output variable, Response variable.
- **Role**: It's the outcome you want your model to learn and predict.
#### Example:
- In predicting house prices:
    - **Dependent Variable**: Price of the house.

### Independent Variable
- **Definition**: The variable(s) used to explain or predict the dependent variable. These are the inputs or features provided to the model.
- **Also Known As**: Predictor variables, Input variables, Features.
- **Role**: They influence or determine the dependent variable.
#### Example:
- In predicting house prices:
    - **Independent Variables**: Square footage, number of bedrooms, location, etc.

In a dataset:
- **Independent Variables**: Columns/features that describe the input data.
- **Dependent Variable**: Column/label that represents the output to be predicted.

#### Example in a Dataset:

| Square Footage | Bedrooms | Location | Price   |
| -------------- | -------- | -------- | ------- |
| 1500           | 3        | Urban    | 200,000 |

- **Independent Variables**: Square Footage, Bedrooms, Location.
- **Dependent Variable**: Price.

### Train Data & Test Data
In machine learning, data is split into two main subsets: **train data** and **test data**. These subsets are crucial for building and evaluating models.

#### Train Data
- **Definition**: The portion of the dataset used to train the machine learning model. The model learns patterns and relationships between input features and the target (dependent) variable from this data.
- **Purpose**: To help the model learn and develop a mathematical representation of the data.
- **Characteristics**:
    - Contains both input features (independent variables) and corresponding output labels (dependent variable).
    - Typically constitutes the majority of the dataset (e.g., 70%-80%).
- **Example**: In a dataset for predicting house prices:
    - Train Data: A subset with known house prices and features like square footage, number of bedrooms, and location.

#### Test Data
- **Definition**: The portion of the dataset used to evaluate the model's performance. It checks how well the model can generalise to unseen data.
- **Purpose**: To measure the model's accuracy, robustness, and ability to handle new data.
- **Characteristics**:
    - Contains the same features as the train data but is not shown to the model during training.
    - Typically constitutes the remaining portion of the dataset (e.g., 20%-30%).
    - Labels are known and used to compare predictions against the actual outcomes.
- **Example**: In the house price dataset:
    - Test Data: A subset with unseen house features and known prices, used to test the model's predictions.

### Supervised Machine Learning
- In supervised learning, the model is trained on a **labelled dataset**, meaning each training example includes input features (independent variables) and their corresponding correct output (dependent variable or label).
- The goal is for the model to learn the mapping from inputs to outputs and predict outcomes for unseen data.

#### Classifications in Supervised ML
1. **Regression:**
	- Predicts a continuous value.
	- Examples:
	    - Predicting house prices based on square footage, number of bedrooms, etc.
	    - Forecasting stock prices.
	- Algorithms:
	    - Linear Regression
	    - Support Vector Regression (SVR)
	    - Decision Tree Regression
2. **Classification**
	- Predicts discrete categories or classes.
	- Examples:
	    - Classifying emails as spam or not spam.
	    - Diagnosing a patient as having a disease or not.
	- Algorithms:
	    - Logistic Regression
	    - Support Vector Machines (SVM)
	    - k-Nearest Neighbours (k-NN)
	    - Decision Trees and Random Forests

### Unsupervised Machine Learning
- In unsupervised learning, the model is trained on **unlabelled data**. The algorithm tries to identify hidden patterns, structures, or relationships in the data without specific guidance.
- The goal is to organise data meaningfully or reduce its dimensionality for easier understanding.

#### Classifications in Unsupervised ML
1. **Clustering**
	- Groups data points into clusters based on their similarity.
	- Examples:
	    - Customer segmentation in marketing.
	    - Organising news articles by topic.
	- Algorithms:
	    - k-Means Clustering
	    - Hierarchical Clustering
	    - DBSCAN (Density-Based Spatial Clustering)
2. **Dimensionality Reduction**
	- Reduces the number of features or variables in the dataset while retaining the most important information.
	- Examples:
	    - Visualising high-dimensional data in 2D or 3D.
	    - Compressing images.
	- Algorithms:
	    - Principal Component Analysis (PCA)
	    - t-Distributed Stochastic Neighbour Embedding (t-SNE)
	    - Singular Value Decomposition (SVD)

### Comparison of Supervised and Unsupervised ML

| **Aspect**           | **Supervised Learning**                         | **Unsupervised Learning**                    |
| -------------------- | ----------------------------------------------- | -------------------------------------------- |
| **Input Data**       | Labeled (input-output pairs)                    | Unlabeled (only input features)              |
| **Goal**             | Predict outcomes (Regression/Classification)    | Identify patterns or structures              |
| **Common Use Cases** | Predictive tasks (e.g., house price prediction) | Exploratory tasks (e.g., grouping customers) |
| **Evaluation**       | Accuracy, Precision, Recall, F1 Score           | Intra-cluster similarity, Explained Variance |
### Outliers
**Outliers** are data points in a dataset that deviate significantly from the other observations. They are unusually high or low values that do not fit the general pattern or trend of the data. These values may arise due to variability in measurement, experimental errors, or genuinely rare events.

#### Characteristics of Outliers
1. They are far away from the majority of the data points.
2. They can distort statistical analyses and machine learning moderns
3. They may represent valuable insights or noise, depending on the context.

#### Types of Outliers

1. **Univariate Outliers**:
    - Outliers identified in one-dimensional data.
    - Example: A person's height in a group of adults (e.g., 9 feet tall).
2. **Multivariate Outliers**:
    - Outliers found in multi-dimensional data that deviate in their combination of values.
    - Example: A car with very high horsepower and very low fuel efficiency compared to others.
3. **Global Outliers**:
    - Values that are outliers in the entire dataset.
    - Example: A house priced at $10 million in a dataset where most houses range from $200,000 to $500,000.
4. **Contextual/Conditional Outliers**:
    - Values that are unusual only in a specific context.
    - Example: A temperature of 35°C is normal in summer but an outlier in winter.
5. **Collective Outliers**:
    - A group of data points that deviate significantly but may not be outliers individually.
    - Example: A sudden cluster of low stock prices during a market crash.

#### How to detect Outliers?
- **Visual Methods**:
    - **Box Plot**:
        - Outliers appear as points outside the whiskers of the plot.
    - **Scatter Plot**:
        - Outliers can be identified as points far from the cluster.
    - **Histogram**:
        - Extreme values create distant bars.
- **Statistical Methods**:
    - **Z-Score**:
        - Measures how far a data point is from the mean in terms of standard deviations.
        - Formula: $Z = \frac{{x - \mu}}{{\sigma}}$
        - Outliers typically have $∣Z∣>3$.
    - **IQR (Interquartile Range)**:
        - Outliers are values below $Q1 - 1.5 × \text{IQR}$ or above $Q3 + 1.5 × \text{IQR}$. 
        - $\text{IQR} = Q3 - Q1$, where Q1 and Q3 are the 1st and 3rd quartiles, respectively.

##### Interquartile Range (IQR)
The **Interquartile Range (IQR)** is a measure of statistical dispersion and is used to detect outliers in a dataset. The IQR is calculated as the difference between the third quartile (Q3) and the first quartile (Q1), representing the middle 50% of the data.
###### Steps to detect outliers using IQR
- **Sort the Data**:
    - Arrange the dataset in ascending order.
- **Find the Quartiles**:
    - **First Quartile (Q1)**: The median of the lower half of the data.
    - **Third Quartile (Q3)**: The median of the upper half of the data.
- **Calculate the IQR**:
    - $\text{IQR} = Q3 - Q1$
- **Determine the Outlier Thresholds**:
    - **Lower Bound**: $Q1 - 1.5 \times \text{IQR}$
    - **Upper Bound**: $Q3 + 1.5 \times \text{IQR}$
- **Identify Outliers**:
    - Any data point below the **lower bound** or above the **upper bound** is considered an outlier.

**EXAMPLE**
Let us consider the following dataset:
10, 12, 15, 18, 22, 28, 35, 100

**Step 1:** Sort the data in ascending order.
**Step 2:** Find first quartile (Q1) and third quartile (Q2)
		- Q1: Median of the lower half = $\frac{12 + 15}{2} = 13.5$
		- Q3: Median of the upper half = $\frac{28 + 35}{2} = 31.5$
**Step 3:** Calculate IQR:
		- $\text{IQR} = Q3 − Q1 = 31.5 − 13.5 = 18$
**Step 4:** Calculate **Outlier Thresholds**
		- **Lower Bound**: $Q1 - 1.5 \times \text{IQR} = 13.5 - 1.5 \times 18 = -13.5$
	    - **Upper Bound**: $Q3 + 1.5 \times \text{IQR} = 31.5 + 1.5 \times 18 = 58.5$
**Step 5:** Identify Outliers:
		- Data points below -13.5: None
		- Data points above 58.5: 100
	
	- Hence the outlier here is 100.

##### Box Plot
A **box plot** (also called a box-and-whisker plot) is a graphical representation used to summarizs the distribution of a dataset and identify potential outliers. It provides insights into the dataset's central tendency, spread, and skewness.

###### Key characters of a Box Plot
- **Median (Q2)**:
    - The middle value of the dataset (50th percentile).
    - Represented by a line inside the box.
- **Interquartile Range (IQR)**:
    - The range between the first quartile (Q1) and the third quartile (Q3).
    - Formula: $\text{IQR} = Q3 - Q1$
    - The box spans from Q1 to Q3.
- **Whiskers**:
    - Extend from the box to the smallest and largest values that are within $1.5 \times \text{IQR}$ from Q1 and Q3, respectively.
    - These indicate the "typical range" of the data.
- **Outliers**:
    - Data points outside the range defined by the whiskers:
        - Below $Q1 - 1.5 \times \text{IQR}$
        - Above $Q3 + 1.5 \times \text{IQR}$
    - Represented as individual points or dots beyond the whiskers.

###### Question:
The marks of students in a maths class test are as follows:
75, 80, 85, 92, 65, 78, 88, 98, 70, 72, 89, 91, 76, 79, 83, 89, 77, 87, 81
Find the outliers, if any and then plot the box plot for the dataset

MCA: a data should be mcar format and misisng in random formed.
   
Advantages of CCA (complete case analysis)
1. easy to implement as no data manipulation required
2. preserves variable distribution

Disadvantage of CCA
1. it can exclude a large fraction of the original dataset
2. excluded observation could be informative for the analysis
3. the model in production the model knows how to handle the missing data
4. 